import asyncio
from pyppeteer import launch
import os
import random

async def scrape_page(page_number, seen_links):
    """
    æ¯æ¬¡æŠ“å–ä¸€é¡µåï¼Œå…³é—­æµè§ˆå™¨ï¼Œé¿å… Cloudflare è§¦å‘åçˆ¬è™«ã€‚
    """
    browser = await launch(headless=False, args=['--no-sandbox', '--disable-setuid-sandbox'], executablePath='C:/Program Files/Google/Chrome/Application/chrome.exe')
    page = await browser.newPage()

    # è®¾ç½® User-Agentï¼Œé˜²æ­¢ Cloudflare æ£€æµ‹
    await page.setUserAgent("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36")

    # ä¼ªè£…ä¸ºçœŸå®ç”¨æˆ·
    await page.evaluateOnNewDocument('''() => {
        Object.defineProperty(navigator, 'webdriver', { get: () => undefined });
        window.navigator.chrome = { runtime: {} };
        Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'] });
        Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5] });
    }''')

    # **æ„é€ æ­£ç¡®çš„ URL**
    if page_number == 1:
        url = 'https://www.notino.it/brand-cosmetici-italiani/'
    else:
        url = f'https://www.notino.it/brand-cosmetici-italiani/?f={page_number}-1-2-62760'

    print(f"ğŸ“„ æ­£åœ¨æŠ“å–ç¬¬ {page_number} é¡µï¼š{url}")

    await page.goto(url, {'timeout': 60000, 'waitUntil': 'domcontentloaded'})

    # **æ¨¡æ‹Ÿæ»šåŠ¨åŠ è½½æ›´å¤šäº§å“**
    for _ in range(5):
        await page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
        await asyncio.sleep(random.uniform(3, 5))  # **æ¯æ¬¡æ»šåŠ¨ç­‰å¾… 3-5 ç§’**

    # **æå–äº§å“é“¾æ¥**
    links = await page.evaluate('''() => {
        return Array.from(document.querySelectorAll('div[data-testid="product-container"] a'))
            .map(link => link.href);
    }''')

    # **ä»…ä¿ç•™å”¯ä¸€çš„é“¾æ¥**
    unique_links = [link for link in links if link not in seen_links]
    seen_links.update(unique_links)  # **æ›´æ–°å·²æŠ“å–çš„é“¾æ¥é›†åˆ**

    print(f"ğŸ”— ç¬¬ {page_number} é¡µæ‰¾åˆ° {len(links)} ä¸ªäº§å“é“¾æ¥ï¼Œå…¶ä¸­ {len(unique_links)} ä¸ªæ˜¯æ–°çš„")

    await browser.close()  # **æ¯æ¬¡æŠ“å–å®Œåå…³é—­æµè§ˆå™¨**
    return unique_links

async def main():
    total_pages = 44
    seen_links = set()  # **å­˜å‚¨å·²æŠ“å–çš„é“¾æ¥**
    save_path = r'C:\Users\mark0\Desktop\program\hello'
    os.makedirs(save_path, exist_ok=True)
    file_path = os.path.join(save_path, 'product_links.txt')

    # **ç¡®ä¿æ–‡ä»¶ä¸ºç©º**
    open(file_path, 'w', encoding='utf-8').close()

    for page_number in range(1, total_pages + 1):
        try:
            unique_links = await scrape_page(page_number, seen_links)

            # **éšæœºå»¶è¿Ÿï¼Œé˜²æ­¢ Cloudflare è¿›ä¸€æ­¥è¯†åˆ«**
            wait_time = random.uniform(10, 15)
            print(f"â³ éšæœºç­‰å¾… {wait_time:.2f} ç§’ï¼Œæ¨¡æ‹Ÿäººç±»è¡Œä¸º...")
            await asyncio.sleep(wait_time)

            # **è¿½åŠ å†™å…¥ï¼ˆä¸ä¼šå†™å…¥é‡å¤çš„é“¾æ¥ï¼‰**
            with open(file_path, 'a', encoding='utf-8') as file:
                for link in unique_links:
                    file.write(link + '\n')

        except Exception as e:
            print(f"âŒ æŠ“å–ç¬¬ {page_number} é¡µå¤±è´¥: {e}")

    print(f"âœ… æŠ“å–å®Œæˆï¼Œæ‰€æœ‰å”¯ä¸€é“¾æ¥å·²ä¿å­˜åˆ° {file_path}")

# è¿è¡Œè„šæœ¬
asyncio.get_event_loop().run_until_complete(main())
