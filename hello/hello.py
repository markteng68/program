import asyncio
from pyppeteer import launch
import os

async def scrape_product_links():
    browser = await launch(headless=True, executablePath='C:/Program Files/Google/Chrome/Application/chrome.exe')
    page = await browser.newPage()

    # è®¿é—®ç›®æ ‡ç½‘é¡µï¼Œå¹¶ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½
    url = 'https://www.notino.it/brand-cosmetici-italiani/'
    
    await page.goto(url, {'timeout': 60000, 'waitUntil': 'domcontentloaded'})
    print(" é¡µé¢åŠ è½½å®Œæˆï¼Œç»§ç»­æ‰§è¡Œåç»­ä»£ç ...")


     # è·å–é¡µé¢ä¸Šæ‰€æœ‰çš„é“¾æ¥
    links = await page.evaluate('''() => {
        return Array.from(document.querySelectorAll('a')).map(link => link.href);
    }''')

    print(f"ğŸ”— é¡µé¢ä¸Šæ‰¾åˆ° {len(links)} ä¸ªé“¾æ¥ï¼Œå¼€å§‹æ£€æŸ¥ç›®æ ‡é“¾æ¥...")
    # ç­‰å¾…é¡µé¢å®Œæˆå¯¼èˆªï¼ˆå¯ä»¥ç¡®ä¿ç½‘ç»œè¯·æ±‚åŠ è½½å®Œæˆï¼‰
    # await page.waitForNavigation({'waitUntil': 'networkidle2', 'timeout': 120000})

    # ç­‰å¾…äº§å“åˆ—è¡¨ä¸­çš„é“¾æ¥å…ƒç´ å‡ºç°
    
    await page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
    await asyncio.sleep(5)  # è®©é¡µé¢åŠ è½½æ›´å¤šå†…å®¹

 # æµ‹è¯•æ˜¯å¦å¯ä»¥æ‰¾åˆ°ç‰¹å®šé“¾æ¥
    specific_link = "https://www.notino.it/collistar/special-perfect-body-gel-dimagrante-corpo-anticellulite/"
    link_exists = await page.evaluate(f'''() => {{
        return Array.from(document.querySelectorAll('a')).some(link => link.href === "{specific_link}");
    }}''')

    if link_exists:
        print(f"âœ… æ‰¾åˆ°ç‰¹å®šé“¾æ¥: {specific_link}")
    else:
        print(f"âŒ æœªæ‰¾åˆ°ç‰¹å®šé“¾æ¥: {specific_link}")

    await browser.close()

    await page.waitForSelector('div[data-testid="product-container"] a', { 'timeout': 120000})

    # æå–äº§å“é“¾æ¥
    links = await page.evaluate('''() => {
        return Array.from(document.querySelectorAll('div[data-testid="product-container"] a'))
            .map(link => link.href);
    }''')

    # å¤„ç†ç›¸å¯¹è·¯å¾„ï¼Œç¡®ä¿æ‰€æœ‰é“¾æ¥å®Œæ•´
    base_url = "https://www.notino.it"
    product_links = [link if link.startswith("http") else base_url + link for link in links]

    # ä¿å­˜åˆ°æ–‡æœ¬æ–‡ä»¶
    save_path = r'C:\Users\mark0\Desktop\program\hello'
    os.makedirs(save_path, exist_ok=True)
    file_path = os.path.join(save_path, 'product_links.txt')

    with open(file_path, 'w', encoding='utf-8') as file:
        for link in product_links:
            file.write(link + '\n')

    print(f"æå–åˆ° {len(product_links)} ä¸ªäº§å“é“¾æ¥ï¼Œå·²ä¿å­˜åˆ° {file_path}")

    await browser.close()

# è¿è¡Œè„šæœ¬
asyncio.get_event_loop().run_until_complete(scrape_product_links())
